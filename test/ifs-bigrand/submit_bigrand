#!/bin/bash
# submit using sbatch --export=NAM=$CONFIG submit_bigrand

#SBATCH --qos=np

#SBATCH --qos=np

        # Specifies that your job will run in the queue (Quality Of
        # Service - QoS) 'nf'. This is the default QoS and is suitable
	# for serial and small parallel jobs.

#SBATCH --ntasks=1

        # Specifies that your job will request 1 MPI tasks

#SBATCH --cpus-per-task=128

        # Specifies the number of CPUs per task - the number of 
	# parallel or OpenMP threads per MPI task
#SBATCH --threads-per-core=1

#SBATCH --mem-per-cpu=100

        # Specifies the Consumable Memory per MPI task

#SBATCH --job-name=slurmout_BIG

        # Assigns the specified name to the request

#SBATCH --output=%x.%j.out

        # Specifies the name and location of STDOUT where %x is the job
        # name and  %j is the job-id. The file will be # written in the
        # workdir directory if it is a relative path. If not given, the 
        # default is slurm-%j.out in the workdir.

#SBATCH --error=%x.%j.out

        # Specifies the name and location of STDERR where %x is the job
        # name and  %j is the job-id. The file will be # written in the
        # workdir directory if it is a relative path. If not given, the 
        # default is slurm-%j.err in the workdir.

#SBATCH --time=00:03:00

        # Specifies that your job my run up to HH:MM:SS of wall clock
        # time. The job will be killed if it exceeds this limit. If
        # time is not defined, the default limit for the queue (qos)
        # will be used.
#SBATCH --hint=nomultithread
set -e

module unload prgenv/intel; module load gcc/9.3.0; module load intel-mkl; module load prgenv/expert; module load netcdf4/4.7.4:gnu:9.3
#module load intel/2021.4.0; module load prgenv/intel; module load hpcx-openmpi/2.8.1; module load intel-mkl/19.0.5; module load netcdf4/4.7.4
#export OMP_STACKSIZE=64MB # use with Intel

#export LD_LIBRARY_PATH=/usr/local/apps/netcdf4/4.7.4/INTEL/2021.2/lib/:$LD_LIBRARY_PATH

export LD_LIBRARY_PATH=/home/papu/libunwind/lib/:$LD_LIBRARY_PATH
export GPTL_TIMING=1

export OMP_PLACES=cores
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}


#srun ../../bin/ecrad $NAM $SCRATCH/inputs_BIG.nc tmp.nc
srun make test_big_ecckd_tc
exit 0          # terminate the program, returning 0 (default) as return code 
                # to the system 

# There is one output file produced by this job: 
#
#         pi-omp-intel.$jobid.out
#
# in the working directory specified by the "--chdir" SBATCH option
#
# End of example job 'pi-omp-intel'
